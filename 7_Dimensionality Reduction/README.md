## 降维

在多元线性回归中所采用的技术是特征选择,特征选择减少的是自变量的个数,这一方法不对初始的数据集的数据进行更改.降维所得到的自变
量不包含在原数据的自变量中,而是由数据的特性抽取得到的新的自变量.


### PCA

寻找最大方差所属的方向,运用PCA算法降维后得到的自变量数目p≤原数据自变量个数m,这p个自变量可以较好的解释数据自变量的差异性(方差),无论因变量是多少.因变量不被考虑在内,因此PCA是一个无监督学习模型.

PCA算法过程为:

![image text]()

PCA的过程发生在特征缩放之后,拟合模型之前,PCA可以视作数据预处理.

本例中,数据的自变量是不同红酒的多个化学成分,因变量是客户分类.参数p的确定取决与p能解释多少自变量的方差,需要在编码中先测试再选择函数赋值,初始值等于原自变量数量.本课程的例子确定了2个新自变量,可以拟合56%的方差.


### kenerl-PCA

对线性不可分的数据,将数据投射到高维空间使其线性可分,分类后再将其投射到原维度.本例子使用与Logistic Regression同样的数据集,自变量个数等于原数据自变量个数,因此不需要的计算方差.观察LR产生的图像,观察得到其最佳分类器并不是一条直线,说明原数据是线性不可分的.k-pca处理后,数据变得线性可分.